---
output: 
  pdf_document:
    fig_crop: false
---
```{r, include=FALSE}
knitr::opts_chunk$set(message = FALSE, fig.align = 'center', fig.height = 3.5, fig.width = 5)
library(dplyr)
library(ggplot2)
library(latex2exp)

# global rv generations
I <- 1000
```

\textbf{Problem Set 1} \newline
STAT 536 \newline
Alex Kappes

\textbf{1}. $x \sim \text{Bin}(n, p)$. Show $E[x] = np$ and $\text{var}(x) = np(1-p)$

The pdf of $X$ is characterized as $f(x;n, p) = {n\choose x}p^x(1-p)^{n-x}$ over the support $x = 0, 1, ..., n$ with $p \in (0, 1)$. $E[x]$ and $\text{var}(x)$ will be characterized through the moment generating function $M_x(t)$. 
$$
\begin{aligned}
M_x(t) = E[e^{tx}] &= \sum_{x=0}^n {n\choose x}e^{tx}p^x(1-p)^{n-x} \\[5pt]
&\Rightarrow \sum_{x=0}^n {n\choose x}(pe^t)^x(1-p)^{n-x}.
\end{aligned}
$$
For $\sum_{x=0}^n{n\choose x}p^xq^{n-x}$ = $(p+q)^n$,
$$
M_x(t) = (pe^t + 1-p)^n,
$$
and
$$
\begin{aligned}
\left.\frac{\partial^rM_x(t)}{\partial t^r} \right\rvert_{t=0} &= E[x^r] \\[5pt]
\left.\frac{\partial M_x(t)}{\partial t} \right\rvert_{t=0} &= n(pe^t + 1-p)^{n-1}pe^t \bigg\rvert_{t=0} = np = E[x] \\[5pt]
\left.\frac{\partial^2 M_x(t)}{\partial t^2} \right\rvert_{t=0} &= npe^t(n-1)(pe^t + 1-p)^{n-2}pe^t + npe^t(pe^t + 1-p)^{n-1} \bigg\rvert_{t=0} \\[5pt]
&\Rightarrow np^2(n-1) + np = E[x^2].
\end{aligned}
$$
With $\text{var}(x) = E[x^2] - (E[x])^2$,
$$
\begin{aligned}
\text{var}(x) &= np^2(n-1) + np - (np)^2 \\[5pt]
&\Rightarrow (np)^2 - np^2 + np - (np)^2 \\[5pt]
\text{var}(x) &= np(1-p).
\end{aligned}
$$
The above shows that $E[x] = np$ and $\text{var}(x) = np(1-p)$

\textbf{2a}. Derive MLE $\hat{\lambda}$ for $x \sim \text{Exp}(\lambda)$ with support $x>0$ and $\lambda>0$.
$$
\begin{aligned}
f(x; \lambda) &= \lambda e^{-\lambda x} \\[5pt]
L(\lambda; x) &= \prod_{i=0}^nf(x_i; \lambda) = \lambda^ne^{-\lambda\sum_{i=0}^nx_i} \\[5pt]
\frac{\partial lnL(\lambda; x)}{\partial \lambda} &= \frac{n}{\lambda} - \sum_{i=0}^nx_i = 0 \\[5pt]
&\Rightarrow \hat{\lambda} = \frac{n}{\sum_{i=0}^nx_i} = \frac{1}{\bar{x}}
\end{aligned}
$$

\textbf{2b}. Derive MLE $\hat{\mu}$ for $x \sim \text{N}(\mu, \sigma^2)$ with support $x \in (-\infty, +\infty)$ and $\sigma^2$ known.
$$
\begin{aligned}
f(x; \mu, \sigma^2) &= \frac{1}{\sqrt{2\pi}\sigma}e^{\left\{-\frac{1}{2\sigma^2}(x-\mu)^2\right\}} \\[5pt]
L(\mu, \sigma^2; x) &= \prod_{i=0}^nf(x_i; \mu, \sigma^2) = \text{C}e^{\left\{-\frac{1}{2\sigma^2}\sum_{i=0}^n(x_i-\mu)^2\right\}}\ \text{for C} = \frac{1}{(2\pi)^\frac{n}{2}\sigma^n} \\[5pt]
\frac{\partial lnL(\mu, \sigma^2; x)}{\partial \mu} &= \frac{1}{\sigma^2}\sum_{i=0}^n(x_i - \mu) = 0 \\[5pt]
&\Rightarrow \sum_{i=0}^nx_i - n\mu = 0 \\[5pt]
&\Rightarrow \hat{\mu} = \frac{\sum_{i=0}^nx_i}{n} = \bar{x}
\end{aligned}
$$

\textbf{3}. Let $F^-$ represent the generalized inverse of a cdf $F$. Show that the following two sets are equivalent.
$$
\{(u, x); F^-(u) \leq x\} = \{(u, x); F(x) \geq u\} 
$$
To show set equivalence for any sets $A$ and $B$, where $A=B$, we will show that $A \subseteq B$ and $B \subseteq A$. Let $A \equiv \{(u, x); F^-(u) \leq x\}$ and $B \equiv \{(u, x); F(x) \geq u\}$. Consider
$$
\begin{aligned}
(u_1, x_1) &\in \{(u, x); F^-(u) \leq x\} \\[5pt]
&\Rightarrow F^-(u_1) \leq x_1 \\[5pt]
&\Rightarrow F(F^-(u_1)) \leq F(x_1)
\end{aligned}
$$
where the generalized inverse satisfies $F(F^-(u)) \geq u$ and $F^-(F(x)) \leq x$, with the equalities $u = F(x)$ and $F^-(u) = x$, it is such that $F(F^-(u)) \geq u \Rightarrow F(x) \geq u$. Thus,
$$
\begin{aligned}
F(F^-(u_1)) \geq u_1 &\Rightarrow F(x_1) \geq u_1 \\[5pt]
F(F^-(u_1)) \leq F(x_1) &\Rightarrow F(x_1) \geq u_1 \\[5pt]
(u_1, x_1) \in A &\subseteq \{(u, x); F(x_1) \geq u_1\} \equiv B.
\end{aligned}
$$
To show $B \subseteq A$,
$$
\begin{aligned}
(u_1, x_1) &\in \{(u, x); F(x) \geq u\} \\[5pt]
&\Rightarrow F(x_1) \geq u_1 \\[5pt]
&\Rightarrow F^-(F(x_1)) \geq F^-(u_1) \\[5pt]
F^-(F(x_1)) \leq x_1 &\Rightarrow F^-(u_1) \leq x_1 \\[5pt]
F^-(F(x_1)) \geq F^-(u_1) &\Rightarrow F^-(u_1) \leq x_1 \\[5pt]
(u_1, x_1) \in B &\subseteq \{(u, x); F^-(u_1) \leq x_1\} \equiv A.
\end{aligned}
$$
Thus, for set equivalence $A=B \Rightarrow A \subseteq B$ and $B \subseteq A$, it has been shown that
$$
A \equiv \{(u, x); F^-(u) \leq x\} = \{(u, x); F(x) \geq u\} \equiv B.
$$

\textbf{4}. [R] Given $x \sim \text{Ber}(p)$, simulate $z = \sum_{i=1}^nx_i \sim \text{Bin}(n, p)$ for 1000 realizations of random variable $z \sim \text{Bin}(3, 0.25)$.

We will first simulate bernoulli random variables $x$ from $\text{U}(0,1)$ random draws using the inverse method. Given $F(x) = 1-p$ for $x \in [0,1)$, we define $x = F^{-1}(u)$ for $u \sim \text{U}(0,1)$ with
$$
F^{-1}(u) = \left\{
\begin{array}{lc}
0 & \text{if}\ u \in (0, 1-p] \\
1 & \text{if}\ u \in (1-p, 1)
\end{array}\right.
$$
The algorithm is as follows,
\begin{itemize}
\item[i] Draw $u \sim \text{U}(0,1)$ $n$ times. 
\item[ii] $x_r = 1$ if $u_i > 1-p$, else $x_r = 0$. 
\item[iii] $z_i = \sum_{r=1}^nx_r$. 
\item[iv] Repeat $I$ times for $i = \{1,...,I\}$. 
\end{itemize}

and is implemented in R as follows. \newline

```{r}
z_binom <- function(n, p, I) {
  rv_vec <- c()
  
  for (i in 1:I) {
    u <- ifelse(runif(n) > 1-p, 1, 0)
    rv_vec[i] <- sum(u)
  }
  return(data.frame('z' = rv_vec))
}

p <- 0.25
n <- 3
```

```{r, echo=FALSE}
ggplot(z_binom(n, p, I), aes(z)) +
  geom_bar() +
  labs(title = TeX('$z \\sim$ Bin(3, 0.25) 1000 RV Generation'),
       x = TeX('$z$'),
       y = TeX('Frequency')) +
  theme(plot.title = element_text(family = 'serif', hjust = 0.5),
        axis.title = element_text(family = 'serif'))
```

\textbf{5}. [R] The transformation method will be used to generate 1000 $\text{gamma}(4, 1)$ random variables.

Based on the transformation $y = \beta\sum_{j=1}^\alpha x_j$ for $x_j \sim \text{Exp}(1)$, $y$ then follows a $\text{gamma}(\alpha, \beta)$ distribution. Starting from $u \sim \text{U}(0,1)$, the algorithm is as follows.
\begin{itemize}
\item[i] Simulate $u_j \sim \text{U}(0,1)$ for $x_j = -ln(u_j) \sim \text{Exp}(1)$.
\item[ii] $y_i = \beta\sum_{j=1}^\alpha x_j$.
\item[iii] Repeat $I$ times for $i = \{1,...,I\}$.
\end{itemize}

The algorithm is implemented as follows with $\{\alpha, \beta\} = \{4, 1\}$. \newline

```{r}
y_gam <- function(a, b, I){
  rv_vec = c()
  
  for (i in 1:I){
    x <- -log(runif(a))
    y <- b*sum(x)
    rv_vec[i] <- y
  }
  return(data.frame('y' = rv_vec))
}

alpha <- 4
beta <- 1
```

```{r, echo=FALSE}
ggplot(y_gam(alpha, beta, I), aes(y)) +
  geom_histogram(aes(y = ..density..), alpha = 0.25) +
  geom_density(alpha = 0.35, fill = 'black') +
  labs(title = TeX('$y \\sim$ gamma(4, 1) 1000 RV Generation'),
       x = TeX('$y$'),
       y = '') +
  theme(plot.title = element_text(family = 'serif', hjust = 0.5),
        axis.title = element_text(family = 'serif'))
```

\textbf{6}. $x \sim \text{gamma}(\alpha, \lambda)$ and $y \sim \text{gamma}(\beta, \lambda)$. Show that $z_1 = \frac{x}{x+y} \sim \text{beta}(\alpha, \beta)$, where $z_1$ is independent of $z_2 = x + y \sim \text{gamma}(\alpha +\beta, \lambda)$.

For the independent random variables $x$ and $y$ with densities $f(x; \alpha, \lambda) = \frac{1}{\lambda^\alpha\Gamma(\alpha)}x^{\alpha-1}e^{-\frac{x}{\lambda}}$ and $f(y; \beta, \lambda) = \frac{1}{\lambda^\beta\Gamma(\beta)}y^{\beta-1}e^{-\frac{y}{\lambda}}$, the joint density becomes
$$
f(x, y; \alpha, \beta, \lambda) = \frac{1}{\lambda^{\alpha + \beta}\Gamma(\alpha)\Gamma(\beta)}x^{\alpha-1}y^{\beta-1}e^{-\frac{1}{\lambda}(x+y)}.
$$
Simultaneously solving $z_1$ and $z_2$ for $x$ and $y$ provides $\{x: g^-1(z_1, z_2) = z_1z_2; 0<z_1<1, 0<z_2<\infty \}$ and $\{y: h^-1(z_1, z_2) = z_2(1-z_1); 0<z_1<1, 0<z_2<\infty \}$, with the joint density $f(z_1, z_2; \alpha, \beta, \lambda)$ characterized as
$$
\begin{aligned}
f(z_1, z_2, \alpha, \beta, \lambda) &= f_{x, y}(g^{-1}(z_1,z_2), h^{-1}(z_1,z_2))|J| \\[5pt]
f_{x, y}(g^{-1}(z_1,z_2), h^{-1}(z_1,z_2)) &= \frac{1}{\lambda^{\alpha+\beta}\Gamma(\alpha)\Gamma(\beta)}(z_1z_2)^{\alpha-1}(z_2-z_2z_1)^{\beta-1}e^{-\frac{z_2}{\lambda}} \\[5pt]
|J| &= \left|
\begin{array}{cc}
\frac{\partial x}{z_1} & \frac{\partial x}{z_2} \\
\frac{\partial y}{z_1} & \frac{\partial y}{z_2}
\end{array} \right| = \left| 
\begin{array}{cc} 
z_2 & z_1 \\
-z_2 & 1-z_1
\end{array} \right| =
z_2 \\[5pt]
f(z_1, z_2, \alpha, \beta, \lambda) &= \frac{1}{\lambda^{\alpha+\beta}\Gamma(\alpha)\Gamma(\beta)}z_2^{\alpha+\beta-1}e^{-\frac{z_2}{\lambda}}z_1^{\alpha-1}(1-z_1)^{\beta-1}.
\end{aligned}
$$
The marginal distributions $f(z_1;, \alpha, \beta)$ and $f(z_2; \alpha, \beta, \lambda)$ are characterized as
$$
\begin{aligned}
f(z_1;, \alpha, \beta) &= \frac{z_1^{\alpha-1}(1-z_1)^{\beta-1}}{\lambda^{\alpha+\beta}\Gamma(\alpha)\Gamma(\beta)} \int\limits_0^\infty z_2^{\alpha+\beta-1}e^{-\frac{z_2}{\lambda}}dz_2 \\[5pt]
&\Rightarrow \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}z_1^{\alpha-1}(1-z_1)^{\beta-1}\int\limits_0^\infty \frac{1}{\lambda^{\alpha+\beta}\Gamma(\alpha+\beta)}z_2^{\alpha+\beta-1}e^{-\frac{z_2}{\lambda}}dz_2 \\[5pt]
&\Rightarrow \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}z_1^{\alpha-1}(1-z_1)^{\beta-1}\int\limits_0^\infty f(z_2; \alpha+\beta, \lambda)dz_2 \\[5pt]
f(z_1;, \alpha, \beta) &= \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}z_1^{\alpha-1}(1-z_1)^{\beta-1} \Rightarrow z_1 \sim \text{beta}(\alpha, \beta)
\end{aligned}
$$
and
$$
\begin{aligned}
f(z_2; \alpha+\beta, \lambda) &= \frac{z_2^{\alpha+\beta-1}e^{-\frac{z_2}{\lambda}}}{\lambda^{\alpha+\beta}\Gamma(\alpha)\Gamma(\beta)}\int\limits_0^1z_1^{\alpha-1}(1-z_1)^{\beta-1}dz_1 \\[5pt]
&\Rightarrow \frac{z_2^{\alpha+\beta-1}e^{-\frac{z_2}{\lambda}}}{\lambda^{\alpha+\beta}\Gamma(\alpha+\beta)}\int\limits_0^1\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}z_1^{\alpha-1}(1-z_1)^{\beta-1}dz_1 \\[5pt]
&\Rightarrow \frac{z_2^{\alpha+\beta-1}e^{-\frac{z_2}{\lambda}}}{\lambda^{\alpha+\beta}\Gamma(\alpha+\beta)}\int\limits_0^1f(z_1; \alpha, \beta)dz_1 \\[5pt]
f(z_2; \alpha+\beta, \lambda) &= \frac{1}{\lambda^{\alpha+\beta}\Gamma(\alpha+\beta)}z_2^{\alpha+\beta-1}e^{-\frac{z_2}{\lambda}} \Rightarrow z_2 \sim \text{gamma}(\alpha+\beta, \lambda).
\end{aligned}
$$
Using the marginal distributions shown above, independence is satisfied through the equality
$$
\begin{aligned}
f(z_1, z_2; \alpha, \beta, \lambda) &= f(z_1; \alpha, \beta)f(z_2; \alpha+\beta, \lambda) \\[5pt]
&\Rightarrow \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}z_1^{\alpha-1}(1-z_1)^{\beta-1}\frac{1}{\lambda^{\alpha+\beta}\Gamma(\alpha+\beta)}z_2^{\alpha+\beta-1}e^{-\frac{z_2}{\lambda}} \\[5pt]
f(z_1, z_2; \alpha, \beta, \lambda) &= \frac{1}{\lambda^{\alpha+\beta}\Gamma(\alpha)\Gamma(\beta)}z_2^{\alpha+\beta-1}e^{-\frac{z_2}{\lambda}}z_1^{\alpha-1}(1-z_1)^{\beta-1}.
\end{aligned}
$$
In conclusion, the above shows that starting from $x \sim \text{gamma}(\alpha, \lambda)$ and $y \sim \text{gamma}(\beta, \lambda)$, $z_1 = \frac{x}{x+y}$ is distributed as $\text{beta}(\alpha, \beta)$ and $z_2 = x + y$ is distributed as $\text{gamma}(\alpha +\beta, \lambda)$ with $z_1$ and $z_2$ being independent.

\textbf{7}. Develop the Accept-Reject method for generating a random sample of size $n$ from target density $f(x)$, $x \sim \text{beta}(c+1, c+1)$ for $c>0$. The instrumental density $g(x)$ is provided from $y \sim \text{U}(0,1)$. The random sample is generated such that the target density $f(x)$ and the instrumental density $g(x)$ satisfy
$$
f(x) \leq Mg(x) \Rightarrow \underset{x, f(x)>0}{\text{sup}}\frac{f(x)}{g(x)} \leq M.
$$
Define $h(x) = \frac{f(x)}{g(x)}$. For $\underset{x, f(x)>0}{\text{sup}}h(x)$,
$$
\begin{aligned}
\text{ln}h(x) &= \text{ln}f(x) - \text{ln}g(x) \\[5pt]
&\Rightarrow \text{ln}\left(\frac{\Gamma(2c+2)}{\Gamma(c+1)\Gamma(c+1)}\right) + c\text{ln}x + c\text{ln}(1-x) \\[5pt]
\frac{\partial \text{ln}h(x)}{\partial x} &= \frac{c}{x} - \frac{c}{1-x} = 0 \Rightarrow \underset{x}{\text{argmax}}\ \text{ln}h(x) = \frac{1}{2}.
\end{aligned}
$$
To find the optimal value for constant $M$ we define $M(b) = h(x)\rvert_{x=1/2}$, where we minimize with respect to $b$. 
$$
\begin{aligned}
\text{ln}M(b) &= \text{ln}\left(\frac{\Gamma(2c+2)}{\Gamma(c+1)\Gamma(c+1)}\right) + 2c\text{ln}\frac{1}{2} + \text{ln}(b-a) \\[5pt]
-\frac{\partial \text{ln}M(b)}{\partial b} &= -\frac{1}{b-a} = 0,\ b>a
\end{aligned}
$$
$\underset{b}{\text{min}}\ \text{ln}M(b)$ provides a trivial solution, where the equality is satisfied only as $b\rightarrow \infty$. However, because the target density support, $x \in (0,1)$, must be contained in the instrumental density support, our attention can be restricted to four instrumental density support cases to find an optimal $b$ value for minimizing the expression $\text{ln}(b-a)$ in $\text{ln}M(b)$.
\begin{itemize}
\item[i] $(0,1) \subseteq (a_1, b_1)$ for $-\infty < a_1 < b_1 < \infty$, providing the value $\text{ln}(b_1 - a_1) \rightarrow \infty$.
\item[ii] $(0,1) \subseteq (a_1, 1)$ for $-\infty < a_1 < 1$, providing the value $\text{ln}(1 - a_1) \rightarrow \infty$.
\item[iii] $(0,1) \subseteq (0, b_1)$ for $0 < b_1 < \infty$, providing the value $\text{ln}(b_1 - 0) \rightarrow \infty$. 
\item[iv] $(0,1) \subseteq (0, 1)$ for $\{a, b\} = \{0, 1\}$. The expression $\text{ln}(b-a)$ subject to $(0,1) \subseteq (a, b)$ is minimized when $\{a, b\} = \{0, 1\}$, providing the value $\text{ln}(1-0) = 0$.
\end{itemize}
Thus for $\{a, b\} = \{0, 1\}$, the constant $M(b) = \frac{\Gamma(2c+2)}{\Gamma(c+1)\Gamma(c+1)}\left(\frac{1}{2}\right)^{2c} \equiv M$. To simulate $x$ from the target density $f(x)$, using the expressions found above, the Accept-Reject algorithm proceeds below.
\begin{itemize}
\item[i] Generate $x \sim g$, $u \sim \text{U}(0, 1)$.
\item[ii] Accept $y = x$ if $u \leq \frac{f(x)}{Mg(x)}$.
\item[iii] Return to (i) otherwise.
\end{itemize}

\textbf{8a}. [R] Simulation of 1000 random variables following a $\text{N}(3, \sqrt{2})$ distribution using the Box-Mueller transform method proceeds with the algorithm

\begin{itemize}
\item[i] Generate $u_1$ and $u_2$ from $\text{U}(0,1)$.
\item[ii] Set
$$
\begin{aligned}
r_1 &= \sqrt{-2ln(u_1)}\text{cos}(2\pi u_2) \\[5pt]
r_2 &= \sqrt{-2ln(u_1)}\text{sin}(2\pi u_2)
\end{aligned}
$$
where $\{r_1, r_2\} = \{z_1, z_2\} \sim \text{N}(0,1)$. 
\item[iii] Repeat $\frac{I}{2}$ times for $i = \{1,...,I\}$ random draws.
\item[iv] Generate $x_i \sim \text{N}(\mu, \sigma^2)$ from $x_i = \mu + \sigma z_i$
\end{itemize}

and is implemented in R as follows for $\mu = 3$ and $\sigma^2 = \sqrt{2}$. \newline

```{r}
norm_boxmuell <- function(mu, sig, I){
  t <- proc.time()
  z_vec <- c()
  
  for (i in 1:I) {
    u_vec <- runif(2)
    x_vec <- c(sqrt(-2*log(u_vec[1]))*cos(2*pi*u_vec[2]),
               sqrt(-2*log(u_vec[1]))*sin(2*pi*u_vec[2]))
    z_vec[i] <- ifelse(1 %% 2 == 0, x_vec[2], x_vec[1])
  }
  
  bm_x_vec <- mu + sig^(1/4)*z_vec
  boxmuell_elapsed <- proc.time() - t
  out <- list('df' = data.frame('x' = bm_x_vec), boxmuell_elapsed)
  return(out)
}

mu <- 3
sig4 <- 2
```

```{r, echo=FALSE}
ggplot(norm_boxmuell(mu, sig4, I)[['df']], aes(x)) +
  geom_histogram(aes(y = ..density..), alpha = 0.25) +
  geom_density(alpha = 0.35, fill = 'black') +
  labs(title = TeX('$x \\sim$ N(3, $\\sqrt{2}$) 1000 RV Generation'),
       subtitle = TeX('Box-Mueller Transform'),
       x = TeX('$x$'),
       y = '') +
  theme(plot.title = element_text(family = 'serif', hjust = 0.5),
        plot.subtitle = element_text(family = 'serif', hjust = 0.5),
        axis.title = element_text(family = 'serif'))
```

\textbf{8b}. [R] The Marsaglia's polar method algorith is as follows.

\begin{itemize}
\item[i] Generate $u_1$ and $u_2$ from $\text{U}(-1,1)$.
\item[ii] Set $S = u_1^2 + u_2^2$ subject to $u_1^2 + u_2^2 < 1$.
\item[iii] Set
$$
\begin{aligned}
r_1 &= \sqrt{-2ln(S)}\frac{u_1}{\sqrt{S}} \\[5pt]
r_2 &= \sqrt{-2ln(S)}\frac{u_2}{\sqrt{S}}
\end{aligned}
$$
where $\{r_1, r_2\} = \{z_1, z_2\} \sim \text{N}(0,1)$. 
\item[iii] Repeat $\frac{I}{2}$ times for $i = \{1,...,I\}$ random draws.
\item[iv] Generate $x_i \sim \text{N}(\mu, \sigma^2)$ from $x_i = \mu + \sigma z_i$
\end{itemize}

The Marsaglia's polar method is implemented in R below. \newline

```{r}
norm_marspm <- function(mu, sig, I) {
  t <- proc.time()
  z_vec <- c()
  
  i <- 0
  while (i < I/2) {
    u_vec <- runif(2, -1, 1)
    s <- sum(u_vec^2)
    if (s < 1) {
      x1 <- sqrt(-2*log(s))*(u_vec[1]/sqrt(s))
      x2 <- sqrt(-2*log(s))*(u_vec[2]/sqrt(s))
      z_vec <- rbind(z_vec, x1, x2)
      i <- i + 1
    } else {
      i <- i
    }
  }
  
  x_marspm_vec <- mu + sig^(1/4)*z_vec
  marspm_elapsed <- proc.time() - t
  out <- list('df' = data.frame('x' = x_marspm_vec), marspm_elapsed)
  return(out)
}
```

```{r, echo=FALSE}
ggplot(norm_marspm(mu, sig4, I)[['df']], aes(x)) +
  geom_histogram(aes(y = ..density..), alpha = 0.25) +
  geom_density(alpha = 0.35, fill = 'black') +
  labs(title = TeX('$x \\sim$ N(3, $\\sqrt{2}$) 1000 RV Generation'),
       subtitle = TeX('Marsaglia\'s Polar Method'),
       x = TeX('$x$'),
       y = '') +
  theme(plot.title = element_text(family = 'serif', hjust = 0.5),
        plot.subtitle = element_text(family = 'serif', hjust = 0.5),
        axis.title = element_text(family = 'serif'))
```

\textbf{8c}. [R] Comparison of processing time between the Box-Mueller transform method and Masaglia's polar method results in the following being more efficient: \newline

```{r}
marspm_t <- round(norm_marspm(mu, sig4, I)[2][[1]][['elapsed']], 5)
bm_t <- round(norm_boxmuell(mu, sig4, I)[2][[1]][['elapsed']], 5)

if (marspm_t < bm_t) {
  cat('Masaglia\'s polar method is more effeicient.\nProcessing time:',
      marspm_t, '<', bm_t)
} else {
  cat('Box-Mueller transform method is more efficient.\nProcessing time:',
      bm_t, '<', marspm_t)
}
```
(which may be due to how the Marsaglia's polar algorithm was programmed, where we would expect it to be more efficient because it does not deal with sin and cos functions.)


\textbf{9}. Develop the Accept-Reject method for the target density $f(x)$ from $x \sim \text{gamma}(a, b)$ and the instrumental density $g(x)$ from $y \sim \text{Exp}(\lambda)$. For a constant $M \geq 1$, $\frac{f(x)}{g(x)} \leq M$. Consider
$$
\begin{aligned}
h(x) &= \frac{f(x)}{g(x)} = \frac{b^a}{\Gamma(a)\lambda}x^{a-1}e^{-x(b-\lambda)} \\[5pt]
\text{ln}h(x) &= \text{ln}\left(\frac{b^a}{\Gamma(a)}\right) - \text{ln}\lambda + (a-1)\text{ln}x -x(b-\lambda) \\[5pt]
\frac{\partial \text{ln}h(x)}{\partial x} &= \frac{a-1}{x} -(b-\lambda) = 0 \Rightarrow \underset{x}{\text{argmax}}\ \text{ln}h(x) = \frac{a-1}{b-\lambda} \\[5pt]
\text{ln}M(\lambda) &= \text{ln}h(x)\rvert_{x = \frac{a-1}{b-\lambda}} = \text{ln}\left(\frac{b^a}{\Gamma(a)}\right) - \text{ln}\lambda + (a-1)\text{ln}\left(\frac{a-1}{b-\lambda}\right) - (a-1) \\[5pt]
\frac{\partial \text{ln}M(\lambda)}{\partial \lambda} &= -\frac{1}{\lambda} + \frac{a-1}{b-\lambda} = 0 \Rightarrow \underset{\lambda}{\text{argmin}}\ \text{ln}M(\lambda) = \frac{b}{a} \\[5pt]
\end{aligned}
$$
resulting in $M(\lambda)\rvert_{\lambda = \frac{b}{a}} = \frac{b^a}{\Gamma(a)}\left(\frac{a}{b}\right)^ae^{-(a-1)} \equiv M$. Using the expressions above, the Accept-Reject algorithm is described below.
\begin{itemize}
\item[i] Generate $x \sim g$, $u \sim \text{U}(0, 1)$.
\item[ii] Accept $y = x$ if $u \leq \frac{f(x)}{Mg(x)}$.
\item[iii] Return to (i) otherwise.
\end{itemize}





























