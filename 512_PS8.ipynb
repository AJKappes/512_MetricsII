{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alex Kappes <br>\n",
    "Problem Set 8 <br>\n",
    "EconS 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 1** \n",
    "\n",
    "Generated data for this problem represents quantity demanded of good $g$ for the $i$th individual, where $i=1,...,100$. Quantity demanded $\\mathbf{y}$, own-price $\\mathbf{x}_1$, cross-price $\\mathbf{x}_2$ for good $j$, and income $\\mathbf{I}$ are observed. However, it is assumed there exists an endogenous preference shifting parameter based on social influence $\\gamma_i$. A strong and valid instrument $\\phi_i$ exists and is characterized as the popularity of good $g$ within individual $i$'s network. The numerical values for this instrument can be thought of as the positive, nuetral, and negative mention-rate for good $g$ within the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "### data generation ###\n",
    "\n",
    "# real parameter values\n",
    "b0 = 3      # intercept\n",
    "b_unobs = 1.5   # unobserved preference effect\n",
    "b1 = -1.25  # own price effect\n",
    "b2 = 1.75   # cross price effect\n",
    "b3 = 2      # income effect\n",
    "\n",
    "n = 100\n",
    "\n",
    "# think of the endogenous treatement as some preference shifting parameter\n",
    "x_unobs = np.concatenate((np.zeros(int(n/2)), stats.norm.rvs(size=int(n/2), loc=1, scale=.1)))\n",
    "\n",
    "# think of the IV as some mention effect - i.e. positive, neutral, negative network mention\n",
    "x_t_iv = np.concatenate((stats.norm.rvs(size=int(n/2), loc=0, scale=0.2),\n",
    "                        stats.norm.rvs(size=int(n/2), loc=1, scale=0.2)))\n",
    "\n",
    "# binary treatment specification\n",
    "x_t = np.concatenate((np.zeros(int(n/2)), np.ones(int(n/2))))\n",
    "\n",
    "x1 = stats.norm.rvs(size=n, loc=10, scale =0.5) # own price\n",
    "x2 = stats.norm.rvs(size=n, loc=12, scale=0.5)  # cross price\n",
    "x3 = stats.norm.rvs(size=n, loc=500, scale=10)  # income\n",
    "\n",
    "y = b0 + b1*x1 + b2*x2 + b3*x3 + (b_unobs * x_unobs + stats.norm.rvs(n)) # quantity demanded\n",
    "y = pd.DataFrame({'y': y})\n",
    "\n",
    "X_iv = pd.DataFrame({'ones': np.ones(n),\n",
    "                     'iv': x_t_iv,\n",
    "                     'x1': x1,\n",
    "                     'x2': x2,\n",
    "                     'x3': x3})\n",
    "\n",
    "X_endog = pd.DataFrame({'ones': np.ones(n),\n",
    "                        'endog': x_unobs,\n",
    "                        'x1': x1,\n",
    "                        'x2': x2,\n",
    "                        'x3': x3})\n",
    "\n",
    "def est_b(y, X):\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y).reshape(n, 1)\n",
    "    return np.linalg.inv(X.T * X) * X.T * y\n",
    "\n",
    "def ehat(y, X):\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y).reshape(n, 1)\n",
    "    e = y - X * est_b(y, X)\n",
    "    return pd.DataFrame(e).rename(columns={0: 'e'})\n",
    "\n",
    "def pred(y, X):\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y).reshape(n, 1)\n",
    "    predict = X * est_b(y, X)\n",
    "    return pd.DataFrame(predict).rename(columns={0:'predict'})\n",
    "\n",
    "# bias estimation\n",
    "iv_tilde = ehat(X_iv['iv'], X_iv[['ones', 'x1', 'x2', 'x3']]).rename(columns={'e': 'val'})\n",
    "\n",
    "lam = (np.cov(iv_tilde['val'], y['y']) / np.var(iv_tilde['val']))[0, 1]\n",
    "\n",
    "iv_hat = pred(X_iv['iv'], X_iv[['ones', 'x1', 'x2', 'x3']]).rename(columns={'predict': 'val'})\n",
    "y_hat = pred(y, X_iv[['ones', 'x1', 'x2', 'x3']]).rename(columns={'predict': 'val'})\n",
    "\n",
    "bias = ((np.var(X_iv['iv']) / (lam * np.var(iv_tilde['val']))) * np.var(ehat(y, X_endog))[0] *\n",
    "        np.cov(iv_hat['val'], y_hat['val']) / np.var(y_hat['val']))[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**\n",
    "\n",
    "Bias is specified as plim$(\\hat{\\delta} - \\delta) = \\frac{Cov(\\overset{\\sim}{\\boldsymbol{\\phi}}, \\boldsymbol{\\varepsilon})}{\\lambda var(\\overset{\\sim}{\\boldsymbol{\\phi}})}$, where\n",
    "\\begin{align*}\n",
    "\\overset{\\sim}{\\boldsymbol{\\phi}} &= \\boldsymbol{\\phi} - \\mathbf{X}_{IV}\\hat{\\boldsymbol{\\beta}} \\\\[7pt]\n",
    "\\boldsymbol{\\varepsilon} &= \\mathbf{y} - \\mathbf{X}_{endog}\\hat{\\boldsymbol{\\beta}} \\\\[7pt]\n",
    "\\lambda &= \\frac{Cov(\\boldsymbol{\\phi}, \\mathbf{y})}{var(\\boldsymbol{\\phi})}.\n",
    "\\end{align*}\n",
    "\n",
    "$\\mathbf{X}_{IV}$ and $\\mathbf{X}_{endog}$ represent the instrumental variable model matrix and the endogenous model matrix, respectively.\n",
    "\n",
    "Extending the bias estimation from $\\frac{Cov(\\overset{\\sim}{\\boldsymbol{\\phi}}, \\boldsymbol{\\varepsilon})}{\\lambda var(\\overset{\\sim}{\\boldsymbol{\\phi}})}$, we can alternatively specify\n",
    "\\begin{equation*}\n",
    "\\frac{Cov(\\overset{\\sim}{\\boldsymbol{\\phi}}, \\boldsymbol{\\varepsilon})}{var(\\overset{\\sim}{\\boldsymbol{\\phi}})} = \\frac{Cov(\\hat{\\boldsymbol{\\phi}}, \\hat{\\mathbf{y}}_{-\\gamma})}{var(\\hat{\\mathbf{y}}_{-\\gamma})},\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\hat{\\boldsymbol{\\phi}} = \\mathbf{X}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\boldsymbol{\\phi}$ and $\\hat{\\mathbf{y}}_{-\\gamma} = \\mathbf{X}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y}$, with $\\mathbf{X} = [\\mathbf{x}_1, \\mathbf{x}_2, \\mathbf{x}_3]$. Extending the notation further, we find that the bias can be represented as\n",
    "\n",
    "\\begin{equation*}\n",
    "\\text{plim}(\\hat{\\delta} - \\delta) = \\frac{Cov(\\overset{\\sim}{\\boldsymbol{\\phi}}, \\boldsymbol{\\varepsilon})}{\\lambda var(\\overset{\\sim}{\\boldsymbol{\\phi}})} = \\frac{var(\\boldsymbol{\\phi})}{\\lambda var(\\overset{\\sim}{\\boldsymbol{\\phi}})}var(\\boldsymbol{\\varepsilon})\\left[\\frac{Cov(\\hat{\\boldsymbol{\\phi}}, \\hat{\\mathbf{y}}_{-\\gamma})}{var(\\hat{\\mathbf{y}}_{-\\gamma})}\\right].\n",
    "\\end{equation*}\n",
    "\n",
    "The change in bias between using a binary treatment IV and a continuous IV is shown by the use of fitted values $\\hat{\\boldsymbol{\\phi}}$ and $\\hat{\\mathbf{y}}_{-\\gamma}$ in the $Cov(\\cdot)$ calculation instead of the conditional binary treatment expectations $\\text{E}[\\mathbf{X}\\boldsymbol{\\beta}\\ \\vert\\ \\phi_i = 1] - \\text{E}[\\mathbf{X}\\boldsymbol{\\beta}\\ \\vert\\ \\phi_i = 0]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** IV estimates are produced below for the specification $y_i = \\beta_0 + \\alpha \\phi_i + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\beta_3 x_{3i} + \\varepsilon_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The IV estimates are: [[103.805], [1.238], [-1.222], [1.741], [1.995]]\n"
     ]
    }
   ],
   "source": [
    "print('The IV estimates are:', np.round(est_b(y, X_iv), 3).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** The estimated bias for the endogenous parameter estimate is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimated bias is 1.0239752087392823e-25\n"
     ]
    }
   ],
   "source": [
    "print('The estimated bias is', bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, the estimated bias and endogenous parameter estimate $\\hat{\\delta}$ are shown to approximately equate the real parameter value of $\\delta = 1.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4999999999983302"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est_b(y, X_endog)[1, 0] + bias"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
